{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "753e3222-ddf9-418d-bc8d-849fdeaa57cd",
      "metadata": {
        "id": "753e3222-ddf9-418d-bc8d-849fdeaa57cd",
        "outputId": "bbc8ad79-3952-451a-b77e-555804c5c4d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'SemiSeg' already exists and is not an empty directory.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'Semi_supervised_v2'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-546833172.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Semi_supervised_v2.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mSemi_supervised_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Semi_supervised_v2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "!git clone https://github.com/TonyYangHan/SemiSeg\n",
        "import sys\n",
        "sys.path.append('/content/Semi_supervised_v2.py')\n",
        "import Semi_supervised_v2\n",
        "import os, cv2, torch, numpy as np, matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from Semi_supervised_v2 import MultiScaleUNet as MSU\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cat computer_vision.yaml\n",
        "!pip install opencv-python numpy torch torchvision scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fc040bdb-a66e-455a-a240-20221101768f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "fc040bdb-a66e-455a-a240-20221101768f",
        "outputId": "75d3d3b0-bdaa-4d6a-f165-b717e627800a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1572829160.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Opt for the best hardware available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_path\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/best_student_v2.pth\"\u001b[0m \u001b[0;31m# The model you want to load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_img_dir\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/1 cell\"\u001b[0m \u001b[0;31m# The directory to brightfield images you would like to test the model with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_mask_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/1 cell\"\u001b[0m \u001b[0;31m# The directory to masks (black-and-white images) you generated as ground truth for the model to learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Opt for the best hardware available\n",
        "model_path    = \"/content/best_student_v2.pth\" # The model you want to load\n",
        "test_img_dir  = \"/content/drive/MyDrive/1 cell\" # The directory to brightfield images you would like to test the model with\n",
        "test_mask_dir = \"/content/drive/MyDrive/1 cell\" # The directory to masks (black-and-white images) you generated as ground truth for the model to learn\n",
        "\n",
        "visualize_n = 5 # How many image-truth-prediction pair you would like to visualize\n",
        "img_size = (512,512) # Resize images to this size\n",
        "thresh = 0.5 # pixels with probability exceeding this will be predicted as organoid present. Otherwise, not present."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63b00dfb-7134-4327-ac30-e05f3b10e007",
      "metadata": {
        "id": "63b00dfb-7134-4327-ac30-e05f3b10e007"
      },
      "source": [
        "**Please make sure you set the parameters in the cell above correctly before proceeding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "b9330283-c59c-4d39-a66f-cbf821beae8c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9330283-c59c-4d39-a66f-cbf821beae8c",
        "outputId": "bd846182-7e23-4310-eb7d-f406ac041586"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiScaleUNet(\n",
              "  (e1): MultiScaleBlock(\n",
              "    (c1): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (c3): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (c5): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (c7): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (fuse): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (e2): MultiScaleBlock(\n",
              "    (c1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (c3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (c5): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (c7): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (fuse): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (e3): MultiScaleBlock(\n",
              "    (c1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (c3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (c5): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (c7): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (fuse): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (e4): MultiScaleBlock(\n",
              "    (c1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (c3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (c5): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (c7): Conv2d(256, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (fuse): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
              "  (d3): MultiScaleBlock(\n",
              "    (c1): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (c3): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (c5): Conv2d(768, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (c7): Conv2d(768, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (fuse): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (d2): MultiScaleBlock(\n",
              "    (c1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (c3): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (c5): Conv2d(384, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (c7): Conv2d(384, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (fuse): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (d1): MultiScaleBlock(\n",
              "    (c1): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (c3): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (c5): Conv2d(192, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (c7): Conv2d(192, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (fuse): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (out): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "# Load the model\n",
        "model = MSU()\n",
        "model.load_state_dict(torch.load(model_path, map_location=device, weights_only = True))\n",
        "model.to(device).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "21ec26d6-76c4-497d-8593-6ca3ab5055de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "21ec26d6-76c4-497d-8593-6ca3ab5055de",
        "outputId": "de1468d8-d728-4baf-fc83-98950f2f516a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'm_bin' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-44-1958782128.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# flatten for metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_bin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'm_bin' is not defined"
          ]
        }
      ],
      "source": [
        "# 3. Helper to preprocess an image for your model\n",
        "def preprocess(img_bgr):\n",
        "    # img_bgr: H×W×3 uint8\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    img_rgb = cv2.resize(img_rgb, img_size)\n",
        "    # normalize to [0,1], then to tensor C×H×W\n",
        "    tensor = torch.from_numpy(img_rgb.astype(np.float32)/255.0).permute(2,0,1)\n",
        "    # add batch dim\n",
        "    return tensor.unsqueeze(0).to(device)\n",
        "\n",
        "# 4. Loop over your test set\n",
        "y_true = []\n",
        "y_pred = []\n",
        "to_visualize = []\n",
        "\n",
        "for fname in sorted(os.listdir(test_img_dir)):\n",
        "    img_path  = os.path.join(test_img_dir,  fname)\n",
        "    mask_path = os.path.join(test_mask_dir, fname)\n",
        "    # load\n",
        "    img  = cv2.imread(img_path)\n",
        "    m    = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "    m    = cv2.resize(m, img_size)\n",
        "\n",
        "    # inference\n",
        "    inp = preprocess(img)\n",
        "    with torch.no_grad():\n",
        "        logit = model(inp)              # [1,1,H,W]\n",
        "        prob  = torch.sigmoid(logit)    # [1,1,H,W]\n",
        "        pred  = (prob > thresh).cpu().numpy()[0,0].astype(np.uint8)\n",
        "\n",
        "    # flatten for metrics\n",
        "    y_true.extend(m_bin.flatten().tolist())\n",
        "    y_pred.extend(pred.flatten().tolist())\n",
        "\n",
        "    # store for viz\n",
        "    if len(to_visualize) < visualize_n:\n",
        "        to_visualize.append((img, m_bin, pred))\n",
        "\n",
        "# 5. Classification report\n",
        "print(classification_report(y_true, y_pred,\n",
        "                            labels=[0,1],\n",
        "                            target_names=[\"background\",\"foreground\"],\n",
        "                            zero_division=0))\n",
        "\n",
        "# 6. Visualization\n",
        "fig, axes = plt.subplots(3, visualize_n, figsize=(4*visualize_n, 12))\n",
        "for i, (img, gt, pr) in enumerate(to_visualize):\n",
        "    # Original\n",
        "    axes[0,i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    axes[0,i].set_title(\"Image\")\n",
        "    axes[0,i].axis(\"off\")\n",
        "    # Ground truth\n",
        "    axes[1,i].imshow(gt, cmap=\"gray\")\n",
        "    axes[1,i].set_title(\"GT Mask\")\n",
        "    axes[1,i].axis(\"off\")\n",
        "    # Prediction\n",
        "    axes[2,i].imshow(pr, cmap=\"gray\")\n",
        "    axes[2,i].set_title(\"Pred Mask\")\n",
        "    axes[2,i].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3972e977-1f0c-4c6a-9d88-4af07b80be7d",
      "metadata": {
        "id": "3972e977-1f0c-4c6a-9d88-4af07b80be7d"
      },
      "source": [
        "**Note: You will see some prediction is slightly off because this is unprocessed predictions.** <br>\n",
        "**Raw predictions should be clearned afterwards in the post-processing pipeline (included in Detection.ipynb)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffe0b0ad-a984-41c1-abb3-c87d274f7dbd",
      "metadata": {
        "id": "ffe0b0ad-a984-41c1-abb3-c87d274f7dbd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}